{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Seasonal Regression for Portland General Electric's Demand\n",
    "\n",
    "This notebook aims to employ simple methods to forecast day-ahead demand for Portland General Electric, while\n",
    "accounting for the weather at Portland International Airport. \n",
    "I'll play with a couple exponential smoothing methods, while accounting for the seasonal patterns. \n",
    "\n",
    "The electricity demand series shows daily, weekly, and annual oscillations.  At a short time scale, I aim to capture the first two of these.\n",
    "I'l assume that any decent updating algorithm should follow the annual oscillations. \n",
    "\n",
    "The approaches I've seen suggest Fourier Series (harmonic regression), linear regression, and Seasonal ARIMA.\n",
    "I've settled on using a seaonal Holt-Winters smoothing, since it's a simple method with a clear way of handling the seasonality.\n",
    "It is also fairly intuitive, there is a base rate, with some daily pattern superimposed.  I'll extend this to also include the\n",
    "non-linear temperature model. \n",
    "It may be possible to also use a multiple seasonal ARIMA approach, by considering yesterday's and last weeks generation.  \n",
    "\n",
    "This notebook is an attempt to build a simple, model including the daily and weekly seasonality, along with temperature data.\n",
    "As the explorations in \"EBA_explore\" found, demand varies most strongly with the deviation of the temperature from some optimum:\n",
    "basically people like it to be around $15-20$ degrees Celcius, and will cool or heat their environment to keep that.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "autoscroll": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "What are the existing methods for doing this?\n",
    "There must be some reviews on this, as well as readily applicable techniques. \n",
    "* ARMA - need to remove obvious daily/weekly seasonality? I really need an ARMAX model, to include temperature.\n",
    " ARMA models require stationarity, and I'm currently tying myself up in knots trying to remove the daily oscillations.\n",
    " One approach is to just consider one hour at a time? \n",
    "\n",
    "* Gradient Boosted Regression Trees (Winning technique at GEFCOM 2014 forecasting competition)\n",
    "* Recurrent Neural Network - train a neural network.  Feed it time-of-day, temperature, weekend/holiday.  (I have a small play network going.)\n",
    "\n",
    "I'll train the models on the 2016 data, and validate on the 2017 data.\n",
    "I'll use the RMSE or MSE as a metric for success.\n",
    "\n",
    "I'll use a persistance forecast as my baseline, so tomorrow's demand is the same as today's. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Multiple Seasonal Exponential Smoothing\n",
    "\n",
    "This follows Rob Hyndman's approach towards multi-seasonal exponential smoothing (https://robjhyndman.com/papers/multiseasonal.pdf).\n",
    "This approach generalizes Holt-Winters smoothing to multiple seasons.\n",
    "Hyndman's analysis includes forecasts of electricity generation, based on utility data (from well over 10 years ago).\n",
    "I believe an improved version was used in a later paper which also included Fourier series to handle electrical demand in Turkey. \n",
    "\n",
    "I chose to follow this model since most attempts at ARIMA rely on removing the seasonality, and I had hoped to just follow best practice with existing libraries.\n",
    "Initial naive methods yield poor results, and failed to remove the seasonal pattern, or even worse imposed one.\n",
    "An initial attempt at Fourier filtering on over a year of data also left similar trends.\n",
    "Perhaps the Fourier approach could be salvaged via more sensible windowing functions. \n",
    "\n",
    "The simple model for a variable $y_{t}$, with seasonal pattern with period $m$ is\n",
    "\\begin{align}\n",
    "  y_{t} &= l_{t-1} + b_{t-1} + s_{t-m} + \\epsilon_{t} \\\\\n",
    "  l_{t} &= l_{t-1} + \\alpha\\epsilon_{t} \\\\\n",
    "  b_{t} &= b_{t-1} + \\beta \\epsilon_{t} \\\\\n",
    "  s_{t} &= s_{t-m} + \\gamma \\epsilon_{t}\n",
    "\\end{align}\n",
    "where $l_{t}, b_{t}, s_{t}$ are the level, trend and seasonal patterns respectively.\n",
    "The noise/innovation $\\epsilon_{t}$ is assumed to be i.i.d. Gaussian and obey\n",
    "$E[\\epsilon_{t}]=0, E[\\epsilon_{t}\\epsilon_{s}]=\\delta_{ts}\\sigma^2$.\n",
    "The innovation is estimated as the difference between the actual result and the predicted result, $\\epsilon_t=\\hat{y}_t-y_t$.\n",
    "The constants $\\alpha,\\beta,\\gamma$ are between zero and one, and determined by fitting to past data.\n",
    "(Hyndman notes that $m+2$ estimates must be made for the initial values of the level, trend and seasonal pattern).\n",
    "\n",
    "Hyndman's model allows multiple seasons, and allows the sub-seasonal terms to be updated more quickly than once per large season.  In utility data, the short season is the daily oscillation, while the longer season comes from the weekly oscillation induced by the work week.  For hourly data, the daily cycle has length $m_{1}=24$, with the weekly cycle taking $m_{2}=168$.  The ratio between them is $k=m_{2}/m_{1}=7.$  The number of seasonal patterns is $r\\le k$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "(I'm going to change Hyndman's notation to use $\\mathbf{I}$ to denote indicator/step functions).\n",
    "\\begin{align}\n",
    "  y_{t} &= l_{t-1}+b_{t-1} +\\sum_{i=1}^{r} \\mathbf{I}_{t,i}s_{i,t-m_1} +\\epsilon_{t}\\\\\n",
    "  l_{t} &= l_{t-1} + b_{t-1}+\\alpha\\epsilon_{t}\\\\\n",
    "  b_{t} &= b_{t-1} + \\beta\\epsilon_{t}\\\\\n",
    "  s_{i,t} &= s_{i,t-m_1} + \\sum_{j=1}^{r}\\left(\\gamma_{ij}\\mathbf{I}_{t,j}\\right) \\epsilon_{t}  (i=1,2,\\ldots,r) \\\\\n",
    "\\end{align}\n",
    "Here the indicator functions $\\mathbf{I}_{t,i}$ are unity if $t$ is in the seasonal pattern $i$, and zero otherwise.  For utility data, this will probably be weekday and holiday/weekend.  Here $\\gamma_{ij}$ denotes how much one seasonal pattern is updated based on another---Hyndman proposes a number of restrictions on these parameters.\n",
    "\n",
    "I will extend this to include an external variables for the deviation above a given temperature, so that $y_{t}\\rightarrow y_{t}+\\tau_p\\Theta(T_t-T_p)+\\tau_{n}\\Theta(T_n-T_t)$.\n",
    "This model allows different coefficients for energy use as a function of heating and cooling, as well as a flat region.\n",
    "I will put restrictions on $T_{p}$, $T_{n}$, so that these are in reasonable ranges, $T_{p}>10,T_{n}$, $T_n<\n",
    "\n",
    "He suggests using the first four weeks of data to estimate the parameters, by minimizing the squared error of the one-step ahead forecast.  Apparently maximum likelihood estimation was not recommended (10 years ago).\n",
    "\n",
    "So how to fit the parameters?  A really simple approach would be gradient descent?  Intuitively, the level is the average value, the bias is the average gradient.  The seasonality is the average seasonal pattern.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from util.get_weather_data import convert_isd_to_df, convert_state_isd\n",
    "from util.EBA_util import remove_na, avg_extremes\n",
    "\n",
    "from numpy import pi,e\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read in PDX Frame from file\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    #load in preprepared joint dataset.  \n",
    "    df_joint=pd.read_csv('data/pdx_joint.txt',\n",
    "        index_col=0, parse_dates=True)\n",
    "    print('Read in PDX Frame from file')\n",
    "    dem=df_joint['Demand'].copy()\n",
    "    temp=df_joint['Temp'].copy()\n",
    "    fore=df_joint['Forecast'].copy()\n",
    "except:\n",
    "    air_df = pd.read_csv('data/air_code_df.gz')\n",
    "    #Just get the weather station data for cities in Oregon.\n",
    "    df_weather=convert_state_isd(air_df,'OR')\n",
    "    #Select temperature for Portland, OR\n",
    "    msk1=np.array(df_weather['city']=='Portland')\n",
    "    msk2=np.array(df_weather['state']=='OR')\n",
    "    df_pdx_weath=df_weather.loc[msk1&msk2]\n",
    "    #get electricity data for Portland General Electric\n",
    "    df_eba=pd.read_csv('data/EBA_time.gz',index_col=0,parse_dates=True)\n",
    "    msk=df_eba.columns.str.contains('Portland')\n",
    "    df_pdx=df_eba.loc[:,msk]\n",
    "    msk1=  df_pdx.columns.str.contains('[Dd]emand') \n",
    "    dem=df_pdx.loc[:,msk1]\n",
    "    #Make a combined Portland Dataframe for demand vs weather.\n",
    "    df_joint=pd.DataFrame(dem)\n",
    "    df_joint=df_joint.join(df_pdx_weath)\n",
    "    temp=df_joint['Temp']\n",
    "    df_joint['TempShift']=150+abs(temp-150)\n",
    "    df_joint = df_joint.rename(columns={df_joint.columns[0]:'Demand',\n",
    "             df_joint.columns[1]:'Forecast'})\n",
    "    df_joint.to_csv('data/pdx_joint.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of extreme values 0. Number of zero values 148\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of NA values 56\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of extreme values 1. Number of zero values 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of NA values 156\n"
     ]
    }
   ],
   "source": [
    "#clea data: remove NA values by replacing with median.  Then average down 4sigma excursions by their nearest neighbours.\n",
    "dem = remove_na(dem,window=24)\n",
    "dem = avg_extremes(dem)\n",
    "temp = avg_extremes(remove_na(temp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "I am currently replacing NA values with the average of the neighbouring days.  This manages to handle the occasional missing day, which is surprisingly common.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Optimizing Hyperparameters\n",
    "\n",
    "The following code tries to optimize the hyperparameters.  It seems to lock onto essentially a persistence model.  The best guess is to just use yesterday's electricity usage to predict today's. Considering that was going to be my benchmark for simple and effective, this is somewhat disheartening.\n",
    "\n",
    "I tried this numerically rather than via directly coding up gradient descent, since the closed form derivatives become nasty polynomials in the parameters $(\\alpha,\\beta,\\gamma)$. The previous values $ y_{t-1} $ also depend on the choice of parameters, so when you consider the derivative of the cost function\n",
    "$\\partial_\\alpha J = T^{-1}\\sum_t \\epsilon_{t}\\partial_\\alpha\\epsilon_{t}$ there is a nested sum of terms (since the parameter estimates rely on the previous errors too)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from EBA_seasonal.multiseasonal import multiseasonal as ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_alpha 0.99\n",
      "_beta 0.999195020887\n",
      "_g00 0.995069964138\n",
      "_g01 0.862099925058\n",
      "_g10 0.662400428419\n",
      "_g11 0.619824581122\n",
      "Failed to hit tolerance after 100 iter\n",
      "\n",
      "Cost: 168.53404623 168.53404623\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7efbc47136a0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost, Old Cost = 168.53404623004775,168.2474619659095\n"
     ]
    }
   ],
   "source": [
    "shift=7\n",
    "start = 24*7*40+shift\n",
    "end =  start+24*7*4+24*7*6\n",
    "#Grab subset, and adjust to timezone.\n",
    "dem_sub = dem[start:end].tz_localize('utc').tz_convert('US/Pacific')\n",
    "temp_sub = temp[start:end].tz_localize('utc').tz_convert('US/Pacific')\n",
    "fore_sub = fore[start:end].tz_localize('utc').tz_convert('US/Pacific')\n",
    "\n",
    "alpha2=0.1\n",
    "beta2=0.1\n",
    "gamma2=np.array([[0.1,0.02],[0.02,0.1]])\n",
    "ms_model2=ms(alpha=alpha2,beta=beta2,gamma=gamma2)\n",
    "=-ypred=ms_model2.optimize_param(dem_sub,rtol=1E-4,nmax=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "So while this hasn't completely converged, the trend is plain enough: this model wants a persistence model, where tomorrow's demand is the same as today's demand.\n",
    "\n",
    "I suspect, this is because this model has no access to weather data, which is clearly a strong driver. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Note: Naively the fitting routine expects each segment of 24 entries to run over one day.  Violating this assumption leads to exponentially growing errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7efbad06e668>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(est.T)\n",
    "plt.plot(dem_sub[-2*168:].index.hour,dem_sub[-2*168:].values,'.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7efbad122780>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(dem_sub[-168:],label='demand')\n",
    "plt.plot(ypred[-168:],label='Predicted')\n",
    "plt.plot(fore_sub[-168:],label='Forecast')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Let's now look at our errors. From eyeballing the last week it's clear\n",
    "that my method is pretty weak, and underperforms relative to the actual PGE forecast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6727"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "mape = lambda x,y: np.mean(np.abs(1-(x)/(y+1E-16)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mape(np.array([1.0,2.0]),np.array([2.0,4.0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My predicted error: 168.53404623004775\n",
      "Actual forecast error: 72.4106044369237\n"
     ]
    }
   ],
   "source": [
    "#Only compare results after initial lock-in period.\n",
    "#So drop first 4 weeks\n",
    "skip=4*168\n",
    "pred_rmse=ms_model2.rmse(ypred[skip:],dem_sub[skip:])\n",
    "fore_rmse=ms_model2.rmse(fore_sub[skip:],dem_sub[skip:])\n",
    "\n",
    "print('My predicted error: {}'.format(pred_rmse))\n",
    "print('Actual forecast error: {}'.format(fore_rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1957.,  1805.,  1728., ...,  2330.,  2350.,  2217.])"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Persistence percentage error: 0.05323434359150992\n",
      "My predicted percentage error: 0.04841695438230639\n",
      "Actual forecast percentage error: 0.02462616675367612\n"
     ]
    }
   ],
   "source": [
    "pers_mape = mape(dem_sub[(skip-24):-24].values, dem_sub[skip:].values)\n",
    "pred_mape = mape(ypred[skip:], dem_sub[skip:])\n",
    "fore_mape = mape(fore_sub[skip:], dem_sub[skip:])\n",
    "\n",
    "print('Persistence percentage error: {}'.format(pers_mape))\n",
    "print('My predicted percentage error: {}'.format(pred_mape))\n",
    "print('Actual forecast percentage error: {}'.format(fore_mape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "So this model's error is twice the actual forecast error.\n",
    "\n",
    "So possible reasons:\n",
    "- I have only a single series, and a few years of data.\n",
    "\n",
    "I only have access to this one time series, rather than\n",
    "the more detailed info PGE would have to possible segment the total\n",
    "demand.\n",
    "\n",
    "Second, this only has prior demand, rather than current temperature\n",
    "or forecasts.  We know that's a big driver. PGE will have their\n",
    "own weather forecast to use in these models.\n",
    "\n",
    "Third, this is a super simple model.  Unfortunately, it's\n",
    "barely better than persistence, which is unfortunate. \n",
    "\n",
    "I can at least fix the second of those, by incorporating past weather\n",
    "forecast data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Multiple Seasonality and Temperature\n",
    "\n",
    "Let's now try to also include the temperature.  From earlier graphs, we'll try to fit a model of the form $A_0|T-T_0|$, i.e. electricity usage linearly increases the further the temperature is from some ideal temperature.  Let's try to first fit the temperature component, then\n",
    "fit the remainder with the prior exponential smoothing model.\n",
    "\n",
    "I'll assume that heating/cooling might have different coefficients, so the temperature component of the model at time $t$ is\n",
    "\\begin{equation}\n",
    "D_{t} =  a_{0}+ a_{+}[T_{t}-T_{+}]_{+} + a_{-}[T_{-}-T_{t}]_{+},\n",
    "\\end{equation}\n",
    "where $[f]_{+}=f$ if $f>0$, and is zero otherwise.\n",
    "\n",
    "In lieu of having actual forecast data, I'm doing to use the actual recorded temperatures to develop the backcast. I should be able to beat the genuine day-ahead forecast, since those are relying on the weather forecast which has its own errors.  But I'm handicapped by only having a limited subset of data (the aggregate demand for a given company)\n",
    "to forecast on, and guessing about which stations are important. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from EBA_seasonal.multiseasonal_temp import multiseasonal_temp as ms_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "shift=7\n",
    "start = 24*7*24+shift\n",
    "end =  start+24*7*4+24*7*20\n",
    "#Grab subset, and adjust to timezone.\n",
    "dem_sub = dem[start:end].tz_localize('utc').tz_convert('US/Pacific')\n",
    "temp_sub = temp[start:end].tz_localize('utc').tz_convert('US/Pacific')\n",
    "fore_sub = fore[start:end].tz_localize('utc').tz_convert('US/Pacific')\n",
    "\n",
    "alpha2=0.3\n",
    "beta2=0.2\n",
    "gamma2=0.05*np.array([[1,0.8],[0.8,1]])\n",
    "ms_model3=ms_temp(alpha=alpha2,beta=beta2,gamma=gamma2,\n",
    "Ap=6,An=5,Tp=200,Tn=150)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7efbad0a56d8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Plot initial predictions, and temperature model.\n",
    "ytot3=ms_model3.STL_dayahead(dem_sub,temp_sub)\n",
    "temp_model=ms_model3.Tmodel(temp_sub)\n",
    "ms_model3.plot_pred([fore_sub,ytot3,2000+temp_model],['Fore','Pred','Temp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7efbc40526a0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ms_model3.plot_pred([ytot3['2016-01-01']-temp_model['2016-01-01']],['Res'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 100.  Cost 100.38151012168807\n",
      "Failed to hit tolerance after 100 iter\n",
      "\n",
      "Cost: 100.381510122 100.381510122\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7efbac762f28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost, Old Cost = 100.38151012168807,100.39020529122762\n"
     ]
    }
   ],
   "source": [
    "debug=False\n",
    "ypred=ms_model3.optimize_param(dem_sub,temp_sub,rtol=1E-2,nmax=100,lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'An': 3.9267997323239148,\n 'Ap': 6.5272227269487715,\n 'Tn': 150.08626320520531,\n 'Tp': 200.1228669469873,\n 'alpha': 0.16120776194559208,\n 'beta': 0.19301566190153127,\n 'g00': 0.059020723246723576,\n 'g01': 0.049197194546316055,\n 'g10': 0.0054450652090559199,\n 'g11': 0.11453486221668183}"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Current best bet on parameters\n",
    "ms_model3.opt_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7efbc4702978>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pred=ms_model3.STL_dayahead(dem_sub,temp_sub)\n",
    "ms_model3.plot_pred([fore_sub,pred,dem_sub],['Fore','Pred','Demand'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Persistence percentage error: 0.0477042384837031\n",
      "My predicted percentage error: 0.03407852025552736\n",
      "Actual forecast percentage error: 0.024113200544899443\n"
     ]
    }
   ],
   "source": [
    "pers_mape = mape(dem_sub[(skip-24):-24].values, dem_sub[skip:].values)\n",
    "pred_mape = mape(ypred[skip:], dem_sub[skip:])\n",
    "fore_mape = mape(fore_sub[skip:], dem_sub[skip:])\n",
    "\n",
    "print('Persistence percentage error: {}'.format(pers_mape))\n",
    "print('My predicted percentage error: {}'.format(pred_mape))\n",
    "print('Actual forecast percentage error: {}'.format(fore_mape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Trying to start training 10 weeks in (Nov 2015) gets crazy answers.\n",
    "So, for small time periods this seems to work, but I seem to have engineered an unstable algorithm.\n",
    "\n",
    "Some periods are worse.  It seems April 2016 is well behaved, but around the winter holidays yields crazy answers, perhaps due to missing those holidays, and causing the errors to propagate through the rest of the system?\n",
    "\n",
    "This makes the neural network more attractive: throw all of the information at it, and let the algorithm extract the useful correlations\n",
    "from the data. Of course, you have a hard time explaining exactly what the network found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7efbad996d68>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "per=slice('2016-03-04','2016-03-18')\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.plot(ytot3[per],'b',label='My Predicted')\n",
    "plt.plot(fore_sub[per],'g',label='Actual Predicted')\n",
    "plt.plot(dem_sub[per],'r',label='Actual')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Testing the model\n",
    "\n",
    "So after fitting a model, let's try running on the full dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "#Now run on the whole thing\n",
    "test_end=-1\n",
    "dem_test = dem[start:test_end].tz_localize( 'utc').tz_convert('US/Pacific')\n",
    "temp_test = temp[start:test_end].tz_localize( 'utc').tz_convert('US/Pacific')\n",
    "fore_test = fore[start:test_end].tz_localize( 'utc').tz_convert('US/Pacific')\n",
    "\n",
    "#now run the whole thing from start to end.\n",
    "ytot3=ms_model3.STL_dayahead(dem_test,temp_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7efbc4702940>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pred_test=ms_model3.STL_dayahead(dem_test,temp_test)\n",
    "ms_model3.plot_pred([fore_test,pred_test,dem_test],['Fore','Pred','Demand'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Ouch.  So the model was trained on data up to December 2016.\n",
    "Shortly after that period, the model stops tracking the correct results,\n",
    "leading to growing oscillations.  These are eventually damped out, but this is awful performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'An': 3.9267997323239148,\n 'Ap': 6.5272227269487715,\n 'Tn': 150.08626320520531,\n 'Tp': 200.1228669469873,\n 'alpha': 0.16120776194559208,\n 'beta': 0.19301566190153127,\n 'g00': 0.059020723246723576,\n 'g01': 0.049197194546316055,\n 'g10': 0.0054450652090559199,\n 'g11': 0.11453486221668183}"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ms_model3.opt_param"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Let's look more closely at that problem period.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7efbac403f60>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "per=slice('2017-02-01','2017-03-21')\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.plot(ytot3[per],'b',label='My Predicted')\n",
    "plt.plot(fore_test[per],'g',label='Actual Predicted')\n",
    "plt.plot(dem_test[per],'r',label='Actual')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "It looks like I have some errors leading to growing over-corrections.\n",
    "What do I think is going on?\n",
    "\n",
    "- Inconsistent model fitting/generation?  I almost certainly have lingering issues in this model, that may be fitting something inconsistently.  \n",
    "- The model assumes a fixed temperature variation over the year.\n",
    "Maybe energy usage does vary, and this simple model is leading to incorrect results?  If those errors get large enough then they could\n",
    "oscillate? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Extensions\n",
    "\n",
    "So, how would this scale to more ISOs?  With trade between them?\n",
    "Right now this barely fits a single linear model.  I think an extension to more tensor indices for locations would do the job.\n",
    "The update parameters would have to become matrices too.\n",
    "But, I'm not going to do that.  This is currently punishing to run since it is heavily unoptimized in slow python code rather than numpy,\n",
    "and it used a finite-difference for the optimization.  So it requires $(N_P+1)$ runs.  It looks like using numba and jit won't really work, as is, since numba does not play nice with objects/classes.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "name": "EBA_seasonal.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
