{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Seasonal Regression\n",
    "\n",
    "The electricity demand series shows daily, weekly, and annual oscillations.  At a short time scale, I aim to capture the first two of these.\n",
    "\n",
    "The approaches I've seen suggest Fourier Series, linear regression, and Seasonal ARIMA.\n",
    "(I got quite stuck on how to detrend the series in a global fashion.)\n",
    "I will focus on building models on the last two weeks of data, with the goal of predicting electricity demand based on temperature, time of day, and day of week."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Hyndman's Multiple Seasonal Exponential Smoothing\n",
    "\n",
    "This follows Rob Hyndman's approach towards multi-seasonal exponential smoothing.  (This generalizes the apparently well-known Holt-Winters smoothing).\n",
    "His analysis includes forecasts of electricity generation, based on utility data (from well over 10 years ago).\n",
    "\n",
    "I chose to follow this model since initial attempts at ARIMA rely on removing the seasonality, and I had hoped to just follow best practice with existing libraries.  Initial naive methods gave complete crap, and failed to remove the seasonal pattern, or even worse imposed one.  An initial attempt at Fourier filtering on over a year of data also left a \n",
    "\n",
    "Hyndman also seems to be a known author within the field of econometric time-series forecasting.  \n",
    "\n",
    "The original model for a variable $y_t$, with seasonal pattern with period $m$ is\n",
    "\\begin{align}\n",
    "  y_t &= l_{t-1}+b_{t-1} +s_{t-m} +\\epsilon_t\\\\\n",
    "  l_t &= l_{t-1} + \\alpha\\epsilon_t\\\\\n",
    "  b_t &= b_{t-1} + \\beta\\epsilon_t\\\\\n",
    "  s_{t} = s_{t-m} + \\gamma \\epsilon_t\n",
    "\\end{align}\n",
    "where $l_t$, b_t,s_t$ are the level, trend and seasonal patterns respectively.\n",
    "The noise is Gaussian and obeys\n",
    "$E[\\epsilon_t]=0, E[\\epsilon_t\\epsilon_s]=\\delta_{ts}\\sigma^2$, and $\\alpha,\\beta,\\gamma$ are constants between zero and one.  (He notes that $m+2$ estimates must be made for the initial values of the level, trend and seasonal pattern).\n",
    "\n",
    "Hyndman's model allows multiple seasons, and allows the sub-seasonal terms to be updated more quickly than once per large season.  In utility data, the short season is the daily oscillation, while the longer season comes from the weekly oscillation induced by the work week.  For hourly data, the daily cycle has length $m_1=24$, with the weekly cycle taking $m_2=168$.  The ratio between them is $k=m_2/m_1=7.$  The number of seasonal patterns is $r\\le k$.  \n",
    "\n",
    "(I'm going to change Hyndman's notation to use $\\mathbf{I}$ to denote indicator/step functions).\n",
    "\\begin{align}\n",
    "  y_t &= l_{t-1}+b_{t-1} +\\sum_{i=1}^r \\mathbf{I}_{t,i}s_{i,t-m_1} +\\epsilon_t\\\\\n",
    "  s_{i,t} = s_{i,t-m_1} + \\sum_{j=1}^r\\left(\\gamma_{ij}\\mathbf{I}_{t,j}\\right) \\epsilon_t  (i=1,2,\\ldots,r)\n",
    "  l_t &= l_{t-1} + b_{t-1}+\\alpha\\epsilon_t\\\\\n",
    "  b_t &= b_{t-1} + \\beta\\epsilon_t\\\\\n",
    "\\end{align}\n",
    "Here the indicator functions $\\mathbf{I}_{t,i}$ are unity if $t$ is in the seasonal pattern $i$, and zero otherwise.  For utility data, this will probably be weekday and holiday/weekend.  Here $\\gamma_{ij}$ denotes how much one seasonal pattern is updated based on another---Hyndman proposes a number of restrictions on these parameters.\n",
    "\n",
    "I will extend this to include an external variables for the deviation above a given temperature, so that $y_t\\rightarrow y_t+\\tau_p\\Theta(T_t-T_p)+\\tau_{n}\\Theta(T_n-T_t)$.\n",
    "\n",
    "He suggests using the first four weeks of data to estimate the parameters, by minimizing the squared error of the one-step ahead forecast.  Apparently maximum likelihood estimation was not recommended (10 years ago).\n",
    "\n",
    "So how to fit the parameters?  A really simple approach would be gradient descent?  Intuitively, the level is the average value, the bias is the average gradient.  The seasonality is the average seasonal pattern.  (This is the dumb STL decomposition used earlier?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from get_weather_data import convert_isd_to_df, convert_state_isd\n",
    "from EBA_util import remove_na, avg_extremes\n",
    "\n",
    "from numpy import pi,e\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done with Mahlon Sweet Field\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done with Salem Municipal Airport/McNary Field\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done with Portland International Airport\n"
     ]
    }
   ],
   "source": [
    "air_df = pd.read_csv('data/air_code_df.gz')\n",
    "\n",
    "#Just get the weather station data for cities in Oregon.\n",
    "df_weather=convert_state_isd(air_df,'OR')\n",
    "#Select temperature for Portland, OR\n",
    "msk1=np.array(df_weather['city']=='Portland')\n",
    "msk2=np.array(df_weather['state']=='OR')\n",
    "\n",
    "df_pdx_weath=df_weather.loc[msk1&msk2]\n",
    "\n",
    "#get electricity data for Portland General Electric\n",
    "df_eba=pd.read_csv('data/EBA_time.gz',index_col=0,parse_dates=True)\n",
    "msk=df_eba.columns.str.contains('Portland')\n",
    "df_pdx=df_eba.loc[:,msk]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "dem=df_pdx.iloc[:,0]\n",
    "#Make a combined Portland Dataframe for demand vs weather.\n",
    "df_joint=pd.DataFrame(dem)\n",
    "df_joint=df_joint.join(df_pdx_weath)\n",
    "temp=df_joint['Temp']\n",
    "df_joint['TempShift']=150+abs(temp-150)\n",
    "df_joint=df_joint.rename(columns={df_joint.columns[0]:'Demand'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of extreme values 0. Number of zero values 148\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of NA values 56\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of extreme values 1. Number of zero values 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of NA values 156\n"
     ]
    }
   ],
   "source": [
    "#clean up data, remove NA\n",
    "dem = remove_na(dem)\n",
    "dem = avg_extremes(dem)\n",
    "temp = avg_extremes(remove_na(temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "#from EBA_seasonal import multiseasonal as ms\n",
    "#Class for Multiseasonal model.\n",
    "class ms:\n",
    "    def fit_init_params(y,ninit=4*24*7):\n",
    "         \"\"\"fit_init_params(y)\n",
    "         Fits initial parameters for Hyndman's multi-seasonal model to\n",
    "         hourly electricity data.\n",
    "         (My guess on how to do this, similar to naive STL method used in \n",
    "         statstools.timeseries)\n",
    "         Finds level, bias and seasonal patterns based on first 4 weeks of data.  \n",
    "         \"\"\"\n",
    "         ysub = y[0:ninit]\n",
    "         yval = ysub.values\n",
    "         ##average value\n",
    "         l = np.mean(yval)\n",
    "         ##average shift\n",
    "         b = np.mean(np.diff(yval))\n",
    "         ##remove mean pattern, subtract off level, and linear trend.\n",
    "         ysub = ysub-l-b*np.arange(ninit)\n",
    "         #mean seasonal pattern.\n",
    "         #second seasonal pattern is for weekends, with days\n",
    "         #Saturday/Sunday have dayofweek equal to 5 and 6.\n",
    "         #make a mask to select out weekends.\n",
    "         s2 = ysub.index.dayofweek >=5\n",
    "         #select out weekends, and regular days. \n",
    "         y_end = ysub[s2]\n",
    "         y_week=ysub[~s2]\n",
    "         n1 = int(len(y_week)/24)\n",
    "         n2 = int(len(y_end)/24)\n",
    "         s = np.zeros((2,24))\n",
    "         print(n1,n2)\n",
    "         for n in range(n1):\n",
    "              s[0,:] = s[0,:]+y_week[n*24:(n+1)*24]/n1\n",
    "         for n in range(n2):\n",
    "              s[1,:] = s[1,:]+y_end[n*24:(n+1)*24]/n2\n",
    "\n",
    "         return l, b, s\n",
    "\n",
    "         # def predict_stl(l,b,s,timeIndex):\n",
    "         #          \"\"\"predict_stl(l,b,s,timeIndex)\n",
    "         #          Predicts STL time-series for a fixed set of parameters.\n",
    "         #          Not useful.\n",
    "         #          \"\"\"\n",
    "         #          # n1 = int(sum(~msk)/24)         \n",
    "         #          # n2 = int(sum(msk)/24)\n",
    "         #          #Use fact that first sub-season is weekdays in first row.\n",
    "         #          #Use integer conversion of true/false to 0/1.\n",
    "         #          #Then use fact that seasonal patterns are 24 hours long to select right hour.\n",
    "         #          #find weekend/weekedays.  \n",
    "         #          msk=timeIndex.dayofweek>=5\n",
    "         #          trend=l+b*np.arange(len(timeIndex))\n",
    "         #          pred=trend+s[msk.astype(int),timeIndex.hour.values]\n",
    "         #          return pred\n",
    "\n",
    "    def STL_step(l,b,s,alpha,beta,gamma,yhat,ypred,t):\n",
    "         \"\"\"STL_step\n",
    "         Updates time parameters based on differences between predicted \n",
    "         and measured.  \n",
    "         (Not fixing the model parameters for update strength!)\n",
    "         Work in Progress\n",
    "         \"\"\"\n",
    "         eps=(yhat-ypred)\n",
    "         #find seasonal patterns.  \n",
    "         m1 = t.hour                     \n",
    "         mr = int(t.dayofweek>=5)\n",
    "         ynew = l + b + s[mr,m1]\n",
    "         l = l+b+alpha*eps\n",
    "         b = b+beta*eps\n",
    "         #update row, and hour\n",
    "         ds =np.dot(gamma,np.array([~mr,mr]))*eps\n",
    "         s[:,m1] += ds\n",
    "         # global icount\n",
    "         # icount +=1\n",
    "         # if (icount%(24*7)==0):\n",
    "         #          print(t)\n",
    "         #          print(alpha*eps,beta*eps,ds,'\\n')\n",
    "         return l,b,s,ynew\n",
    "\n",
    "    def predict_STL(y,alpha,beta,gamma):\n",
    "         \"\"\"predict_STL\n",
    "         Generates initial parameters, and then predicts remainder\n",
    "         of series for input data y.  \n",
    "         \"\"\"\n",
    "     \n",
    "         ninit=4*24*7\n",
    "         l,b,s=fit_init_params(y,ninit=4*24*7)\n",
    "         t0=y.index[ninit+1]\n",
    "         m1 = t0.dayofweek>=5\n",
    "         m2 = t0.hour\n",
    "         \n",
    "         ypred = l+b*ninit+s[int(m1),m2]\n",
    "         ytot = np.zeros(len(y))\n",
    "         print(s[int(m1),m2])\n",
    "         for i in range(ninit,len(y)):\n",
    "             l,b,s,ypred = STL_step(l,b,s,\n",
    "                            alpha,beta,gamma,\n",
    "                              y[i],ypred,y.index[i])\n",
    "         ytot[i]=ypred\n",
    "         # if i%(24*7) ==0:\n",
    "         #         print(\"l: {} b: {}\\n\".format(str(l),str(b)))\n",
    "         #         print(s,\"\\n\")\n",
    "         ytot=pd.Series(ytot,index=y.index)         \n",
    "         return ytot,l,b,s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatic pdb calling has been turned OFF\n"
     ]
    }
   ],
   "source": [
    "%pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7efbc32f7dd8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 8\n",
      "996.490836349\n"
     ]
    }
   ],
   "source": [
    "dem_sub = dem[0:24*7*20]\n",
    "alpha=0.01\n",
    "beta=0.01\n",
    "gamma=0.1*np.array([[1,0.1],[0.1,1]])\n",
    "global icount\n",
    "icount=0\n",
    "ytot,l,b,s=ms.predict_STL(dem_sub,alpha,beta,gamma)\n",
    "per='2016-01'\n",
    "plt.plot(ytot,'b',label='Predicted')\n",
    "plt.plot(dem_sub,'r',label='Actual')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7efbc32f3278>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jonathan/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: RuntimeWarning: divide by zero encountered in log\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "plt.plot(np.log(np.abs(ytot)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f9e5e386278>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "l,b,s=ms.fit_init_params(dem_sub,ninit=4*24*7)\n",
    "plt.plot(s.T)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Thresholded Temperature Model\n",
    "\n",
    "Let's just try to build a linear model for the temperature on top of this.\n",
    "I'll assume that heating/cooling might have different coefficients, so the temperature component of the model at time $t$ is\n",
    "\\begin{equation}\n",
    "D_t =  a_0+ a_+[T_t-T_{+}]_{+} + a_-[T_{-}-T_t]_{+},\n",
    "\\end{equation}\n",
    "where $[f]_+=f$ if $f>0$, and is zero otherwise.  If we optimize the mean square\n",
    "error, then the components can be found by solving for the values that minimize the derivatives.\n",
    "If criteria is mean square error, then can solve directly for parameters\n",
    "$J = sum_t[\\hat{D}_t-D_t)^2$, where $\\hat{D}_t$ is the true value.\n",
    "We must solve $\\partial J/\\partial \\alpha = 0 \\rightarrow \\sum_t \\frac{\\partial D_t}{\\partial\\alpha}(\\hat{D}_t-D_t) $\n",
    "\n",
    "Those conditions are\n",
    "\\begin{align}\n",
    "    \\sum_t(\\hat{D}_t-D_t)=0  \\qquad (a_0)\\\\\n",
    "    \\sum_{t\\in T_\\pm}[T_t-T_\\pm]_\\pm(\\hat{D}_t-D_t)=0  \\qquad (a_\\pm)\\\\\n",
    "    \\sum_{t\\in T_\\pm}(\\pm a_{\\pm})(\\hat{D}_t-D_t)=0  \\qquad (T_\\pm)\\\\\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "(First OOP class - absolutely ridiculous)  Will just use a pandas Dataframe - want a named set of numbers, with defined operations.  Already here.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "#Experimenting with OOP for making \"vector\" of parameters with named labels.\n",
    "#Feels daft - if there's a less stupid way, I'll try to fix this.  (Just use a Dataframe!?)\n",
    "#Initial attempts at getting smarter initialization (with arguments to make a dict just returned empty.\n",
    "# class param_vec(dict):\n",
    "#     \"\"\"Class for model parameters.\n",
    "#     Stores parameters in dict, with arithmatic operations.\n",
    "#     \"\"\"\n",
    "#     #Define elementwise subtraction/addition\n",
    "#     def __add__(self,x):\n",
    "#         y = self.copy()\n",
    "#         if isinstance(x,(int,float)):\n",
    "#             for i,v in self.items():\n",
    "#                 y[i]=self[i]+x\n",
    "#         else:\n",
    "#             for i,v in self.items():\n",
    "#                 y[i]=self[i]+x[i]\n",
    "#         return param_vec(y)\n",
    "\n",
    "#     #Define elementwise subtraction/addition\n",
    "#     def __radd__(self,x):\n",
    "#         return param_vec.__add__(self,x)\n",
    "    \n",
    "#     #Define elementwise subtraction.\n",
    "#     def __sub__(self,x):\n",
    "#         y=self.copy()\n",
    "#         if isinstance(x,(int,float)):\n",
    "#             for i,v in self.items():\n",
    "#                 y[i]=v-x\n",
    "#         else:        \n",
    "#             for i,v in self.items():\n",
    "#                 y[i]=v-x[i]\n",
    "#         return param_vec(y)\n",
    "                \n",
    "#     #Define elementwise subtraction/addition\n",
    "#     def __rsub__(self,x):\n",
    "#         return param_vec.__sub__(self,x)\n",
    "                \n",
    "#     #Define elementwise subtraction.\n",
    "#     def __truediv__(self,x):\n",
    "#         y=self.copy()\n",
    "#         if isinstance(x,(int,float)):\n",
    "#             for i,v in self.items():\n",
    "#                 y[i]=v/x\n",
    "#         else:        \n",
    "#             for i,v in self.items():\n",
    "#                 y[i]=v/x[i]\n",
    "#         return param_vec(y)\n",
    "                \n",
    "#     #Define elementwise subtraction.\n",
    "#     def __mul__(self,x):\n",
    "#         y=self.copy()\n",
    "#         if isinstance(x,(int,float)):\n",
    "#             for i,v in self.items():\n",
    "#                 y[i]=v*x\n",
    "#         else:        \n",
    "#             for i,v in self.items():\n",
    "#                 y[i]=v*x[i]\n",
    "#         return param_vec(y)\n",
    "                \n",
    "#     #Define elementwise subtraction.\n",
    "#     def __rmul__(self,x):\n",
    "#         return param_vec.__mul__(self,x)\n",
    "\n",
    "def param_vec(names,vals):\n",
    "    p=pd.Series(vals,index=names)\n",
    "    return p\n",
    "\n",
    "#might extend to contain all of the modelling stuff?          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "dem_sub  = dem[0:24*7*1]\n",
    "temp_sub = temp[0:24*7*1]\n",
    "\n",
    "D=dem_sub\n",
    "T=temp_sub\n",
    "Dr = np.max(D)-np.min(D)\n",
    "Tr = np.max(T)-np.min(T)\n",
    "\n",
    "pnames=['a0','ap','an','Tp','Tn']\n",
    "pval=[np.mean(D),0.5*Dr/Tr,0.5*Dr/Tr,200,100]\n",
    "\n",
    "Tmodel=just_temp_model(names=pnames,vals=pval)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "class just_temp_model:\n",
    "    def __init__(self,names=[],vals=[]):\n",
    "        self.param= pd.Series(vals,index=names)\n",
    "\n",
    "    def temp_model(self,T):\n",
    "        \"\"\"temp_model(self,T)\n",
    "        Tries to fit linear model for electricity demand to temperature.\n",
    "        Initially tried to allow thresholding, and different slopes for heating/cooling.  \n",
    "        Will now just try simpler linear model a_p|T-T_p|.\n",
    "        \"\"\" \n",
    "        m1 = T>self.param['Tp']\n",
    "        m2 = T<self.param['Tn']\n",
    "        y=np.zeros(T.shape)\n",
    "        y[m1] = (T[m1]-self.param['Tp'])*self.param['ap']\n",
    "        y[m2] = (self.param['Tn']-T[m2])*self.param['an']\n",
    "        y=y+self.param['a0']\n",
    "        #y = param['a0']+ param['ap']*np.abs(T-param['Tp'])\n",
    "        y=pd.Series(y,name='Predicted Demand',index=T.index)\n",
    "        return y\n",
    "\n",
    "    def temp_model_grad(self,D,Dhat,T):\n",
    "        \"\"\"temp_model_grad\n",
    "        Compute gradients of model w.r.t. parameters.\n",
    "        Assumes loss-function is mean-square.\n",
    "        Dhat - measured demand\n",
    "        D    - predicted demand\n",
    "        T    - measured temperature\n",
    "        \"\"\"\n",
    "        m1 = T>self.param['Tp']\n",
    "        m2 = T<self.param['Tn']\n",
    "        Nt = len(T)\n",
    "        Derr=D-Dhat\n",
    "        #initialize with zeros\n",
    "        dparam=param_vec(self.param.keys(),np.zeros(len(self.param)))\n",
    "        dparam['a0'] = np.sum(Derr)/Nt\n",
    "        #Single model\n",
    "        # dparam['ap'] = np.sum( np.abs(T-param['Tp'])*Derr)/Nt\n",
    "        # dparam['Tp'] = -np.sum(np.sign(T-param['Tp'])*param['ap']*Derr)/Nt\n",
    "        #Double thresholded model\n",
    "        dparam['ap'] = np.sum( np.abs(T[m1]-self.param['Tp'])  \\\n",
    "                     *(Derr[m1]))/Nt\n",
    "        dparam['an'] = np.sum( (self.param['Tn']-T[m2])*(Derr[m2]))/Nt\n",
    "        dparam['Tp'] = -self.param['ap']*np.sum(Derr[m1])/Nt\n",
    "        dparam['Tn'] =  self.param['an']*np.sum(Derr[m2])/Nt    \n",
    "        return dparam\n",
    "    \n",
    "    def param_fit(self,Dhat,T,alpha=0.1,rtol=1E-4,nmax=200):\n",
    "        \"\"\"Try to fit linear threshold model of demand to temperature.\n",
    "            D - demand data\n",
    "            T - temperature data\n",
    "            Fits model of form:\n",
    "            D ~ a_0+ a_p[T-T_p]_+ + a_n[T_n-T]_+,\n",
    "            where [f]_+ =f for f>0, and 0 otherwise.\n",
    "\n",
    "            Just use simple gradient descent to fit the model.\n",
    "        \"\"\"\n",
    "        #make parameter estimates\n",
    "        Dr = np.max(Dhat)-np.min(Dhat)\n",
    "        Tr = np.max(T)-np.min(T)\n",
    "        param_names=['a0','ap','an','Tp','Tn']\n",
    "        param_vals=[np.mean(Dhat), 0.5*Dr/Tr, 0.5*Dr/Tr,\n",
    "        np.mean(T), np.mean(T)]\n",
    "        self.param=param_vec(param_names,param_vals)\n",
    "        Dpred = self.temp_model(T)\n",
    "        J=np.sum((Dpred-Dhat)**2)/len(Dhat)\n",
    "        print('Init cost',J)\n",
    "        plot_pred(Dpred,Dhat,10*T)\n",
    "        print('Param:',self.param,\"\\n\")    \n",
    "\n",
    "        Ni=0\n",
    "        for i in range(nmax):\n",
    "            dparam=self.temp_model_grad(Dpred,Dhat,T)\n",
    "            self.param=self.param-alpha*dparam\n",
    "            Dpred=self.temp_model(T)\n",
    "            J2=J        \n",
    "            J=np.sum((Dpred-Dhat)**2)/len(Dhat)\n",
    "            err_change=abs(1-J2/J)\n",
    "            Ni+=1\n",
    "            if (err_change<rtol):\n",
    "               print(\"Hit tolerance {} at iter {}\".format(\n",
    "               err_change,Ni))\n",
    "               plot_pred(Dpred,Dhat,T)                      \n",
    "               return param,Dpred\n",
    "            if(Ni%100==0):\n",
    "                print(\"Cost, Old Cost = {},{}\".format(J,J2))\n",
    "                print('Param:',self.param)\n",
    "                print('Param_grad:',dparam)\n",
    "                print(\"Mean param Change {} at iter {}\".format(err_change,Ni))\n",
    "                plot_pred(Dpred,Dhat,T)\n",
    "        print(\"Failed to hit tolerance after {} iter\\n\".format(iter))\n",
    "        print(\"Cost:\",J,J2)\n",
    "        return param, Dpred \n",
    "\n",
    "    def grad_check(self,Dhat,T):\n",
    "        \"\"\"grad_check(Dhat,T)\n",
    "        Check numerical gradients against finite difference.\n",
    "            D - demand data\n",
    "            T - temperature data\n",
    "        \"\"\"\n",
    "        #make parameter estimates\n",
    "        Dr = np.max(Dhat)-np.min(Dhat)\n",
    "        Tr = np.max(T)-np.min(T)\n",
    "        param_names=['a0','ap','an','Tp','Tn']\n",
    "        param_vals=1000*np.random.random(size=5)\n",
    "        self.param=param_vec(param_names,param_vals)\n",
    "        print(self.param)\n",
    "        Dpred = self.temp_model(T)    \n",
    "        dparam=self.temp_model_grad(Dpred,Dhat,T)    \n",
    "        J=0.5*np.sum((Dpred-Dhat)**2)/len(Dhat)\n",
    "        eps=.01\n",
    "        param2 = self.param.copy()\n",
    "        for name,val in param2.items():\n",
    "            self.param = param2.copy()\n",
    "            self.param[name]=val+eps\n",
    "            Dpred = self.temp_model(T)\n",
    "            J2=0.5*np.sum((Dpred-Dhat)**2)/len(Dhat)\n",
    "            numgrad = (J2-J)/eps\n",
    "            print(name,numgrad,dparam[name],self.param[name])\n",
    "\n",
    "#End of temp_model class.\n",
    "#Needed to scale the data to get decent answers.  \n",
    "\n",
    "def scale_data(D,T):\n",
    "    \"\"\"scale_data\n",
    "    Scales both demand and temperature to have zero mean, unit standard deviation.\n",
    "    Returns scaled data, as well as means, and standard deviations.\n",
    "    \"\"\"\n",
    "    scaling_df=pd.DataFrame(columns=['dem','temp'],index=['mu','std'])\n",
    "    scaling_df.loc['mu','dem']=D.mean()\n",
    "    scaling_df.loc['std','dem']=D.std()\n",
    "    scaling_df.loc['mu','temp']=T.mean()\n",
    "    scaling_df.loc['std','temp']=T.std()\n",
    "    T_scale=(T-T.mean())/T.std()\n",
    "    D_scale=(D-D.mean())/D.std()\n",
    "    return D_scale,T_scale,scaling_df\n",
    "\n",
    "def plot_pred(Dpred,D,T):\n",
    "    \"\"\"make plot to compare fitted parameters\"\"\"\n",
    "    plt.plot(T,'r',label='temp')    \n",
    "    plt.plot(D,'g',label='obs')\n",
    "    plt.plot(Dpred,'b',label='pred')\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a0    321.518791\n",
      "ap    995.282815\n",
      "an    681.873253\n",
      "Tp    743.912988\n",
      "Tn    201.516651\n",
      "dtype: float64\n",
      "a0 -313657.0816040039 -313657.090282 321.528790544\n",
      "ap 151737230.07202148 0.0 995.292814876\n",
      "an 0.0 -12501.1763538 681.883252594\n",
      "Tp 311551992.6437378 -0.0 743.922987577\n",
      "Tn 0.0 -430751.896026 201.526651372\n"
     ]
    }
   ],
   "source": [
    "dem_sub  = dem[0:20]\n",
    "temp_sub = temp[0:20]\n",
    "#D,T,scale_df=scale_data(dem_sub,temp_sub)\n",
    "Tmodel=just_temp_model(names=pnames,vals=pval)\n",
    "Tmodel.grad_check(dem_sub,temp_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Fourier Series\n",
    "\n",
    "Another approach that I've seen to seasonality is to just use a Fourier series.\n",
    "This is similar to an approach I was using based on trying to filter the Fourier tranformed data.\n",
    "This method tries to fit annual, weekly and daily oscillations to the data. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "#Now to do some simple Fourier Series fitting too.\n",
    "class fourier_model:\n",
    "    def total_fourier_series(self,D,n_max=[2,4,4]):\n",
    "        \"\"\"fourier_series\n",
    "        Fits pandas time series D, with Fourier series. \n",
    "        Uses Pandas DateTimeIndex for times.\n",
    "        Produces fourier series with annual, daily and weekly oscillations to fourier series.\n",
    "        Computes coefficients, and then series.  Returns both\n",
    "        D - demand (values to be fitted)\n",
    "        T - DatetimeIndex\n",
    "        n_max - maximum number of coefficients\n",
    "\n",
    "        Note:Misses Holidays.\n",
    "        \"\"\"\n",
    "        T=D.index\n",
    "        T_dayofyear = T.dayofyear.values\n",
    "        T_dayofweek = T.dayofweek.values\n",
    "        T_hour = T.hour.values\n",
    "        Tfit = [T_dayofyear/365,\n",
    "        (T_dayofweek*24+T_hour)/168,\n",
    "        T_hour/24]\n",
    "        periods=[365,168,24]\n",
    "        Nt = len(D)\n",
    "        ftot = np.zeros(Nt)\n",
    "        coeff=[[np.sum(D)/Nt,0]]    \n",
    "        for i in range(3):\n",
    "            if n_max[i]>0:\n",
    "                 ci    = self.fit_fourier_series(D,Tfit[i],n_max[i])\n",
    "                 ftot += self.fourier_series(ci,Tfit[i])             \n",
    "            else:\n",
    "                ci=None\n",
    "            coeff.append(ci)\n",
    "        #add on constant    \n",
    "        ftot+= coeff[0][0]\n",
    "        ftot=pd.Series(ftot,index=T)         \n",
    "        return ftot,coeff\n",
    "\n",
    "    def fit_fourier_series(self,D,T,nmax):\n",
    "        \"\"\"Fits the Fourier series to data D, on times T,\n",
    "        and returns parameters.\n",
    "        \"\"\"\n",
    "        Nt = len(D)\n",
    "        #initial zero coefficients\n",
    "        coeff=[]\n",
    "        for n in range(1,nmax+1):\n",
    "            an= 2*np.sum(np.cos(2*pi*n*T)*D)/Nt\n",
    "            bn= 2*np.sum(np.sin(2*pi*n*T)*D)/Nt\n",
    "            coeff.append([an,bn])\n",
    "        return coeff                       \n",
    "\n",
    "    def fourier_series(self,coeff,T):\n",
    "        \"\"\"fourier_series\n",
    "        Make simple Fourier series with specified coefficients, \n",
    "        over time T. \n",
    "        T assumed to be in range [0,1).\n",
    "        \"\"\"\n",
    "        i=1\n",
    "        nmax=len(coeff)\n",
    "        f=np.zeros(T.shape)\n",
    "        #need a +1 somewhere due to 0-indexing, and not including constant\n",
    "        for n in range(nmax):\n",
    "            an, bn = coeff[n]\n",
    "            f += an*np.cos(2*pi*(n+1)*T)+bn*np.sin(2*pi*(n+1)*T)\n",
    "        return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatic pdb calling has been turned OFF\n"
     ]
    }
   ],
   "source": [
    "%pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'builtin_function_or_method' object has no attribute 'dayofyear'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-57-4c0d5c2054ee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mnmax\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mDf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDcoeff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfourier_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal_fourier_series\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdem_sub\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnmax\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mplot_pred\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdem_sub\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtemp_sub\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-56-e4828b0b79a2>\u001b[0m in \u001b[0;36mtotal_fourier_series\u001b[0;34m(self, D, n_max)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \"\"\"\n\u001b[1;32m     15\u001b[0m         \u001b[0mT\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mT_dayofyear\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdayofyear\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mT_dayofweek\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdayofweek\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mT_hour\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhour\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'builtin_function_or_method' object has no attribute 'dayofyear'"
     ]
    }
   ],
   "source": [
    "dem_sub  = dem[0:24*7*4]\n",
    "temp_sub = 10*temp[0:24*7*4]\n",
    "\n",
    "nmax = [0,4,4]\n",
    "\n",
    "Df, Dcoeff = fourier_model.total_fourier_series(dem_sub,nmax)\n",
    "plot_pred(Df,dem_sub,temp_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatic pdb calling has been turned ON\n"
     ]
    }
   ],
   "source": [
    "%pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init cost 1.0459630818455434\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f9e60753dd8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Param: {'a0': 2.0403652312679588e-16, 'ap': 0.46708059154688253, 'an': 0.46708059154688253, 'Tp': -7.599740942374584e-18, 'Tn': -7.599740942374584e-18} \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost, Old Cost = 0.2195143515128714,0.2197024082319617\n",
      "Param: {'a0': -0.35311812807232357, 'ap': 0.81915796361757598, 'an': -0.55496339513011805, 'Tp': -0.58384597356876855, 'Tn': -0.10167281635891282}\n",
      "Param_grad: {'a0': -0.0075882356519908645, 'ap': 0.017650932004730162, 'an': 0.021595530350529406, 'Tp': 0.028091509562537323, 'Tn': 0.0089551371980910923}\n",
      "Mean param Change 0.0008566944156234158 at iter 100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f9e605ca5c0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost, Old Cost = 0.21336496095305546,0.21339646918507224\n",
      "Param: {'a0': -0.34032674885453923, 'ap': 0.74556786333968239, 'an': -0.68420417537364742, 'Tp': -0.70298109102774575, 'Tn': -0.19052066162488607}\n",
      "Param_grad: {'a0': 0.0002134943921533122, 'ap': 0.0029321058537893157, 'an': 0.009966485139349059, 'Tp': 0.0066009118309116226, 'Tn': 0.0095204945162540022}\n",
      "Mean param Change 0.00014767294440498624 at iter 200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f9e5fe42eb8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to hit tolerance after <built-in function iter> iter\n",
      "\n",
      "Cost: 0.21336496095305546 0.21339646918507224\n"
     ]
    }
   ],
   "source": [
    "dem_scale,temp_scale,scale_df=scale_data(dem_sub,temp_sub)\n",
    "plt.figure()\n",
    "param,Dpred=param_fit(dem_scale,temp_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 8\n"
     ]
    }
   ],
   "source": [
    "l,b,s=fit_init_params(dem_sub)\n",
    "\n",
    "pred=predict_stl(l,b,s,dem_sub.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f9e6058f630>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(dem_sub.values,'b',temp_sub.values,'r',pred,'k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Rambling Time!\n",
    "\n",
    "From a Kalman filter perspective, I think that sometimes the error/innovation terms can be written as $\\epsilon_t = y_t-\\hat{y}_t$, where $y_t$ is the actual value, and $\\hat{y}_t$ is the output of the model with no noise.  The innovation process, then gives a rule for updating (the set of parameters $\\alpha,\\beta,\\Gamma, l, b, s_{i,t}$) how to change in the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatic pdb calling has been turned OFF\n"
     ]
    }
   ],
   "source": [
    "%pdb"
   ]
  }
 ],
 "metadata": {
  "name": "EBA_seasonal.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
