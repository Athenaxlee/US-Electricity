{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Recurrent Neural Network\n",
    "\n",
    "Let's try throwing a neural network at this problem. (This was my ultimate goal all along.)  We'll give input method the day of the week, time of day, day of the year, and temperature.    This first version uses a single recurrent cell, with a linear combination at the end.  This could be enhanced by making deeper networks at both the beginning and end, using a fancier cell (LSTM, GRU).  \n",
    "\n",
    "The networks will be trained on one year's worth of data, and then tested on the remainder.\n",
    "\n",
    "This desperately needs some regularization (dropout), as it is overfitting the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from util.get_weather_data import convert_isd_to_df, convert_state_isd\n",
    "from util.EBA_util import remove_na, avg_extremes\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.layers import fully_connected\n",
    "from tensorflow.contrib.rnn import BasicRNNCell,LSTMCell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read in PDX Frame from file\n"
     ]
    }
   ],
   "source": [
    "#Extend to multiple temperature series\n",
    "try:\n",
    "    df_joint=pd.read_csv('data/pdx_joint.txt',\n",
    "        index_col=0, parse_dates=True)\n",
    "    print('Read in PDX Frame from file')\n",
    "except:\n",
    "    print('Creating PDX DataFrame from scratch')\n",
    "    air_df = pd.read_csv('data/air_code_df.gz')\n",
    "    #Just get the weather station data for cities in Oregon.\n",
    "    df_weather=convert_state_isd(air_df,'OR')\n",
    "    #Select temperature for Portland, OR\n",
    "    #msk1=np.array(df_weather['city']=='Portland')\n",
    "    msk2=np.array(df_weather['state']=='OR')\n",
    "    df_pdx_weath=df_weather.loc[msk2]\n",
    "    #find number of unique station city/state combinations\n",
    "    Nstation = len(df_pdx_weath['city, state'].unique())\n",
    "\n",
    "    #reshape the single temperature column into Nstation copies.  \n",
    "    unique_station=df_pdx_weath['city, state'].unique()\n",
    "    temp_df=pd.DataFrame()\n",
    "    for station in unique_station:\n",
    "        colname=str('Temp-'+station)\n",
    "        temp_df[colname]=df_pdx_weath.loc[df_pdx_weath['city, state']==station,'Temp']\n",
    "\n",
    "    #get electricity data for Portland General Electric\n",
    "    df_eba=pd.read_csv('data/EBA_time.gz',index_col=0,parse_dates=True)\n",
    "    msk=df_eba.columns.str.contains('Portland')\n",
    "    df_pdx=df_eba.loc[:,msk]\n",
    "    #select out demand data\n",
    "    msk1 = df_pdx.columns.str.contains('[Dd]emand') \n",
    "    dem=df_pdx.loc[:,msk1]\n",
    "    #Make a combined Portland Dataframe for demand vs weather.\n",
    "    df_joint=pd.DataFrame(dem)\n",
    "    df_joint=df_joint.join(temp_df)\n",
    "    df_joint = df_joint.rename(columns={df_joint.columns[0]:'Demand',\n",
    "             df_joint.columns[1]:'Forecast'})\n",
    "    df_joint.to_csv('data/pdx_joint.txt')\n",
    "    \n",
    "dem=df_joint['Demand'].copy()\n",
    "temp=df_joint.loc[:,df_joint.columns.str.contains('Temp')].copy()\n",
    "fore=df_joint['Forecast'].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of extreme values 1. Number of zero values 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of NA values 156\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of extreme values 1. Number of zero values 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of NA values 56\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of extreme values 0. Number of zero values 148\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of NA values 56\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of extreme values 1. Number of zero values 189\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of NA values 55\n"
     ]
    }
   ],
   "source": [
    "#clean up data, remove NA\n",
    "#remove NA values, and average extreme values down\n",
    "for y in [temp,dem]:\n",
    "    if len(y.shape)>1:\n",
    "        for i in range(y.shape[1]):\n",
    "            x= y.iloc[:,i]\n",
    "            x = remove_na(x)\n",
    "            y.iloc[:,i] = avg_extremes(x)\n",
    "    else:\n",
    "        x= y\n",
    "        x = remove_na(x)\n",
    "        y = avg_extremes(x)\n",
    "    \n",
    "#if len(temp.shape)>1:\n",
    "#    for i in range(temp.shape[1]):\n",
    "#        x=temp.iloc[:,i]\n",
    "#        x = remove_na(x)\n",
    "#        temp.iloc[:,i] = avg_extremes(x)\n",
    "\n",
    "#temp=remove_na(temp)\n",
    "#temp = avg_extremes(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def make_temptime_data(temp_mat):\n",
    "    \"\"\"make_input_data\n",
    "    Takes input temperature data matrix (for multiple locations),\n",
    "    and extends with extra indices for time of day, day of year, day of week, and holiday. \n",
    "\n",
    "    Input: temp_mat - pandas series of temperatures a location.  \n",
    "    Output: in_mat - scaled matrix of temperatures, and scaled times of day and year.\n",
    "            temp_max - maximum temperature for series (needed to invert transformations?)\n",
    "            temp_min - minimum temperature\n",
    "    \"\"\"\n",
    "    Tind = temp_mat.index\n",
    "    Nt=len(Tind)\n",
    "    hr = Tind.hour.values/(24-1)\n",
    "    #scale length of year\n",
    "    dyear = Tind.dayofyear.values/(365-1+Tind.is_leap_year.astype(int))\n",
    "    dweek = Tind.dayofweek.values/(7-1)\n",
    "    #scale temperature data to so that max/min correspond to [0,1]  \n",
    "    temp_max = temp_mat.max(axis=0)\n",
    "    temp_min = temp_mat.min(axis=0)\n",
    "    temp_mat = (temp_mat-temp_min)/(temp_max-temp_min)\n",
    "    in_mat=np.stack([hr,dweek,dyear]).T\n",
    "    in_mat= np.hstack([temp_mat.values,in_mat])\n",
    "    return in_mat, temp_max,temp_min\n",
    "\n",
    "def scale_demand(dem):\n",
    "    \"\"\"scale_demand\n",
    "    Scale demand to be on 0,1 scale.\n",
    "    Input: demand - series at single location\n",
    "    Output: dem_scale - scaled array of values.\n",
    "            dem_max, dem_min - the maximum and minimum values.\n",
    "    \"\"\"\n",
    "    dem_scale = dem.values\n",
    "    dem_max = np.max(dem_scale)\n",
    "    dem_min = np.min(dem_scale)\n",
    "    dem_scale = (dem_scale-dem_min)/(dem_max-dem_min)\n",
    "    return dem_scale, dem_max,dem_min\n",
    "\n",
    "#drop data prior to \n",
    "temp_mat,tmax,tmin=make_temptime_data(temp)\n",
    "dem_mat,dmax,dmin=scale_demand(dem)\n",
    "\n",
    "Nt=len(dem)\n",
    "Ntest = Nt//2\n",
    "\n",
    "temp_train = temp_mat[0:Ntest,:]\n",
    "temp_test = temp_mat[Ntest:,:]\n",
    "dem_train = dem_mat[0:Ntest]\n",
    "dem_test = dem_mat[Ntest:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20216,)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dem_mat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'temp_mat' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-3b57ab85585f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0mXb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0myb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mget_random_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp_mat\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdem_mat\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m24\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'temp_mat' is not defined"
     ]
    }
   ],
   "source": [
    "def get_random_batch(X,y,n_batch,seq_len):\n",
    "    \"\"\"get_random_batch(Xsig,t,n_batch)   \n",
    "    Gets multiple random samples for the data.\n",
    "    Samples generated by 'get_selection' function.\n",
    "    Makes list of returned entries.\n",
    "    Then combines together with 'stack' function at the end.\n",
    "\n",
    "    X - matrix of inputs, (Nt, Ninputs)\n",
    "    y - vector of desired outputs (Nt)\n",
    "    n_batch - number of batches\n",
    "    seq_len - length of sequence to extract in each batch\n",
    "\n",
    "    Outputs:\n",
    "    X_batch - random subset of inputs shape (Nbatch,seq_len,Ninputs) \n",
    "    y_batch - corresponding subset of outputs (Nbatch,seq_len)\n",
    "    \"\"\"\n",
    "    Nt,Nin = X.shape\n",
    "    x_list=[]\n",
    "    y_list=[]\n",
    "    for i in range(n_batch):\n",
    "        n0=int(np.random.random()*(Nt-seq_len-1))\n",
    "        x_sub = X[n0:n0+seq_len]\n",
    "        y_sub = y[n0:n0+seq_len]\n",
    "        x_list.append(x_sub)\n",
    "        y_list.append(y_sub)\n",
    "    x_batch=np.stack(x_list,axis=0)\n",
    "    y_batch=np.stack(y_list,axis=0)\n",
    "    y_batch=y_batch.reshape( [n_batch,seq_len,-1])                    \n",
    "    return x_batch,y_batch\n",
    "\n",
    "Xb,yb=get_random_batch(temp_mat,dem_mat,1000,24)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "n_steps=24\n",
    "n_inputs=len(temp.iloc[0])+3\n",
    "n_neurons=120\n",
    "n_layers=3\n",
    "n_outputs=1  #number of stations to predict at that time.\n",
    "lr=1E-2\n",
    "np.random.seed(seed=3453)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def make_RNN_cell(n_neurons,fn=tf.nn.relu):\n",
    "    cell=BasicRNNCell(num_units=n_neurons,activation=fn)\n",
    "    return cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE on batch  0 :\t 35.2459\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data\n",
      "Running this thang\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting up graphs:Multi-layer RNN\n"
     ]
    }
   ],
   "source": [
    "#Initial test with code liberally borrowed from ch14 of Geron's \n",
    "#\"Practical Machine Learning with scikit-learn and Tensorflow\"\n",
    "\n",
    "#Makes a single RNN cell, with a fully connected output layer (with no activation on the output).\n",
    "\n",
    "print('setting up graphs:Multi-layer RNN')\n",
    "tf.reset_default_graph()\n",
    "#inputs:  Nobs, with n_steps, and n_inputs per step\n",
    "X = tf.placeholder(tf.float32,[None,n_steps,n_inputs],name='X')\n",
    "#Outputs: n_outputs we want to predict in the future.\n",
    "y = tf.placeholder(tf.float32,[None,n_steps,n_outputs],name='y')\n",
    "\n",
    "#define neural network shape\n",
    "#works:make a list of them.  \n",
    "# cell=BasicRNNCell(num_units=n_neurons,activation=tf.nn.relu)\n",
    "# cell2=BasicRNNCell(num_units=n_neurons,activation=tf.nn.relu)\n",
    "# multi_cell = tf.contrib.rnn.MultiRNNCell([cell,cell2],state_is_tuple=True)\n",
    "\n",
    "#Make a list of cells to pass along.  \n",
    "cell_list=[]\n",
    "for i in range(n_layers):\n",
    "    cell_list.append(make_RNN_cell(n_neurons,tf.nn.relu))\n",
    "\n",
    "multi_cell=tf.contrib.rnn.MultiRNNCell(cell_list,state_is_tuple=True)\n",
    "#multi_cell = tf.contrib.rnn.MultiRNNCell([cell]*n_layers,state_is_tuple=True)\n",
    "#Note that using [cell]*n_layers did not work.  Might need to change init_state?\n",
    "rnn_outputs,states=tf.nn.dynamic_rnn(multi_cell,X,dtype=tf.float32)\n",
    "#this maps the number of hidden units to fewer outputs.\n",
    "stacked_rnn_outputs = tf.reshape(rnn_outputs,[-1,n_neurons])\n",
    "stacked_outputs = fully_connected(stacked_rnn_outputs,n_outputs,activation_fn=None)\n",
    "outputs=tf.reshape(stacked_outputs,[-1,n_steps,n_outputs])\n",
    "\n",
    "#define loss (mean-square-error)\n",
    "loss = tf.reduce_mean(tf.square(outputs-y))\n",
    "#define optimization function.\n",
    "optimizer=tf.train.AdamOptimizer(learning_rate=lr)\n",
    "training_op=optimizer.minimize(loss)\n",
    "init=tf.global_variables_initializer()\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "#compute number correct.\n",
    "print('Loading data')\n",
    "n_iter=10\n",
    "n_batch=100\n",
    "run_network=True\n",
    "\n",
    "if (run_network==True):\n",
    "    print('Running this thang')\n",
    "    with tf.Session() as sess:\n",
    "        init.run()\n",
    "        for iteration in range(n_iter):\n",
    "            #select random starting point. \n",
    "            X_batch,y_batch=get_random_batch(\n",
    "                            temp_train, dem_train, n_batch, n_steps)\n",
    "\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y:y_batch})\n",
    "            if iteration%10 ==0:\n",
    "                mse =loss.eval(feed_dict={X:X_batch,y:y_batch})\n",
    "                print(\"MSE on batch \",iteration,':\\t',mse)\n",
    "                #save model\n",
    "                saver.save(sess, \"./models/pdx_RNN_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "So multiple tanhs are bad.  A couple ReLU layers seem to work well, but do lead to negative predictions.  Note that in comparisons that the early 2015 data is pretty flaky (like the forecasts are zero, and I had to fix multiple issues in the demand data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def model_predict_whole(Xin,path_str=\"pdx_RNN_model\"):\n",
    "    \"\"\"model_predict_whole(tstart)\n",
    "    Retrieve the outputs of the network for all values of the inputs \n",
    "    \"\"\"\n",
    "    Nt,Nin=Xin.shape\n",
    "    nmax = int(Nt/n_steps)\n",
    "    ytot = np.zeros((Nt,1))\n",
    "    #Note that loading/saving graph is not properly implemented yet.    \n",
    "    #reset graph, and reload saved graph\n",
    "    tf.reset_default_graph()\n",
    "    model_path = \"./models/\"+path_str    \n",
    "    saver = tf.train.import_meta_graph(model_path+\".meta\")        \n",
    "    with tf.Session() as sess:\n",
    "\n",
    "        #restore variables\n",
    "        saver.restore(sess,model_path)\n",
    "        for i in range(nmax-1):\n",
    "            n0=n_steps*i\n",
    "            x_sub = Xin[n0:n0+n_steps,:]\n",
    "            x_sub = x_sub.reshape(-1,n_steps,Nin)\n",
    "            y_pred=sess.run(outputs,feed_dict={X:x_sub})\n",
    "            ytot[n0:n0+n_steps]=y_pred\n",
    "    return ytot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def plot_whole_sample_fit(X,y,ntest,n_steps,path_str=\"pdx_RNN_model\"):\n",
    "    \"\"\"plot_whole_sample_fit\n",
    "\n",
    "    Plot ALL of the predictions of the trained model\n",
    "    on a 'test' set with different noise, and longer\n",
    "    times.  Concatenates the predicted results together.  \n",
    "    \"\"\"\n",
    "    #pull in the inputs, and predictions\n",
    "    Nt, Nin = X.shape\n",
    "    ytot=model_predict_whole(X,path_str)\n",
    "    plt.figure()\n",
    "    #now plot against the test sets defined earlier\n",
    "    plt.plot(np.arange(0,ntest),X[:ntest,0],'b',label='Training')\n",
    "    plt.plot(np.arange(ntest,Nt), X[ntest:,0],'g',label='Test')\n",
    "    plt.plot(np.arange(Nt),ytot,'r',label='Predicted')\n",
    "    plt.plot(np.arange(Nt),dem_mat,label='Real')\n",
    "    plt.legend(loc='right')\n",
    "    plt.show()\n",
    "    return ytot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'outputs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-7c1d934eedfa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#n0,x_sub,y_pred=toy_predict(2.5)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mytot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mplot_whole_sample_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp_mat\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdem_mat\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mNtest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'pdx_RNN_model'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-9-9c69ecbe6d13>\u001b[0m in \u001b[0;36mplot_whole_sample_fit\u001b[0;34m(X, y, ntest, n_steps, path_str)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m#pull in the inputs, and predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mNt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mytot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_predict_whole\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpath_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m#now plot against the test sets defined earlier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-c2dc546856d6>\u001b[0m in \u001b[0;36mmodel_predict_whole\u001b[0;34m(Xin, path_str)\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0mx_sub\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mXin\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mn0\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mn_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0mx_sub\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_sub\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mNin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m             \u001b[0my_pred\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mx_sub\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m             \u001b[0mytot\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mn0\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mn_steps\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mytot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'outputs' is not defined"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./models/pdx_RNN_model\n"
     ]
    }
   ],
   "source": [
    "#n0,x_sub,y_pred=toy_predict(2.5)\n",
    "ytot=plot_whole_sample_fit(temp_mat,dem_mat,Ntest,n_steps,'pdx_RNN_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "pred=pd.Series(((dmax-dmin)*ytot+dmin).reshape(-1),index=dem.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7efb9f73c5c0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def rmse(x,y):\n",
    "    z = np.sqrt(np.sum((x-y)*(x-y))/len(x))\n",
    "    return z\n",
    "\n",
    "plt.plot(fore['2015-11':],pred['2015-11':],'.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "174.694761782\n",
      "152.026316468\n"
     ]
    }
   ],
   "source": [
    "nt = len(ytot)//2\n",
    "print(rmse(fore[:nt],dem[:nt]))\n",
    "print(rmse(pred[nt:],dem[nt:]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7efb9ef76470>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "date_slice=slice('2016-12-20','2017-01-02')\n",
    "plt.plot(pred[date_slice],label='pred')\n",
    "plt.plot(dem[date_slice],label='demand')\n",
    "plt.plot(fore[date_slice],label='fore')\n",
    "plt.legend(loc='right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7efb9f20c2b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "date_slice=slice('2016-10-01','2017-01-02')\n",
    "plt.plot(pred[date_slice]/dem[date_slice]-1,label='pred err')\n",
    "plt.plot(fore[date_slice]/dem[date_slice]-1,label='fore err')\n",
    "plt.legend(loc='right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "So looking at the percentage errors, this model (which currently lacks knowledge of holidays) is messing up on Thanksgiving.  Also the model seems to make opposite errors to the forecast model.  It's probably worth checking that the distribution of errors.  Eyeballing the curves shows that the errors are lowest early in the morning, and highest at midday.  The error signal probably has a significant daily frequency component.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "name": "EBA_RNN.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
